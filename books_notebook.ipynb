{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using twitter model to predict the popularity of the books\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from torch import nn, optim\n",
    "import lightning as pl\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-5\n",
    "max_length = 512\n",
    "model_name = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "# model_name = 'MoritzLaurer/deberta-v3-large-zeroshot-v2.0'\n",
    "# model_name = 'dslim/bert-base-NER'\n",
    "# model_name = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Band of Angels: The Untold Story of America...</td>\n",
       "      <td>10.88</td>\n",
       "      <td>2/3</td>\n",
       "      <td>A Great Book about women in WWII</td>\n",
       "      <td>I have alway been a fan of fiction books set i...</td>\n",
       "      <td>In the fall of 1941, the Philippines was a gar...</td>\n",
       "      <td>'Elizabeth Norman'</td>\n",
       "      <td>'History'</td>\n",
       "      <td>Unpopular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prayer That Brings Revival: Interceding for Go...</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0/0</td>\n",
       "      <td>Very helpful book for church prayer groups and...</td>\n",
       "      <td>Very helpful book to give you a better prayer ...</td>\n",
       "      <td>In Prayer That Brings Revival, best-selling au...</td>\n",
       "      <td>'Yong-gi Cho'</td>\n",
       "      <td>'Religion'</td>\n",
       "      <td>Unpopular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Mystical Journey from Jesus to Christ</td>\n",
       "      <td>24.95</td>\n",
       "      <td>17/19</td>\n",
       "      <td>Universal Spiritual Awakening Guide With Some ...</td>\n",
       "      <td>The message of this book is to find yourself a...</td>\n",
       "      <td>THE MYSTICAL JOURNEY FROM JESUS TO CHRIST Disc...</td>\n",
       "      <td>'Muata Ashby'</td>\n",
       "      <td>'Body, Mind &amp; Spirit'</td>\n",
       "      <td>Unpopular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death Row</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0/1</td>\n",
       "      <td>Ben Kincaid tries to stop an execution.</td>\n",
       "      <td>The hero of William Bernhardt's Ben Kincaid no...</td>\n",
       "      <td>Upon receiving his execution date, one of the ...</td>\n",
       "      <td>'Lynden Harris'</td>\n",
       "      <td>'Social Science'</td>\n",
       "      <td>Unpopular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sound and Form in Modern Poetry: Second Editio...</td>\n",
       "      <td>32.50</td>\n",
       "      <td>18/20</td>\n",
       "      <td>good introduction to modern prosody</td>\n",
       "      <td>There's a lot in this book which the reader wi...</td>\n",
       "      <td>An updated and expanded version of a classic a...</td>\n",
       "      <td>'Harvey Seymour Gross', 'Robert McDowell'</td>\n",
       "      <td>'Poetry'</td>\n",
       "      <td>Unpopular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  price  \\\n",
       "0  We Band of Angels: The Untold Story of America...  10.88   \n",
       "1  Prayer That Brings Revival: Interceding for Go...   9.35   \n",
       "2          The Mystical Journey from Jesus to Christ  24.95   \n",
       "3                                          Death Row   7.99   \n",
       "4  Sound and Form in Modern Poetry: Second Editio...  32.50   \n",
       "\n",
       "  review/helpfulness                                     review/summary  \\\n",
       "0                2/3                   A Great Book about women in WWII   \n",
       "1                0/0  Very helpful book for church prayer groups and...   \n",
       "2              17/19  Universal Spiritual Awakening Guide With Some ...   \n",
       "3                0/1            Ben Kincaid tries to stop an execution.   \n",
       "4              18/20                good introduction to modern prosody   \n",
       "\n",
       "                                         review/text  \\\n",
       "0  I have alway been a fan of fiction books set i...   \n",
       "1  Very helpful book to give you a better prayer ...   \n",
       "2  The message of this book is to find yourself a...   \n",
       "3  The hero of William Bernhardt's Ben Kincaid no...   \n",
       "4  There's a lot in this book which the reader wi...   \n",
       "\n",
       "                                         description  \\\n",
       "0  In the fall of 1941, the Philippines was a gar...   \n",
       "1  In Prayer That Brings Revival, best-selling au...   \n",
       "2  THE MYSTICAL JOURNEY FROM JESUS TO CHRIST Disc...   \n",
       "3  Upon receiving his execution date, one of the ...   \n",
       "4  An updated and expanded version of a classic a...   \n",
       "\n",
       "                                     authors             categories popularity  \n",
       "0                         'Elizabeth Norman'              'History'  Unpopular  \n",
       "1                              'Yong-gi Cho'             'Religion'  Unpopular  \n",
       "2                              'Muata Ashby'  'Body, Mind & Spirit'  Unpopular  \n",
       "3                            'Lynden Harris'       'Social Science'  Unpopular  \n",
       "4  'Harvey Seymour Gross', 'Robert McDowell'               'Poetry'  Unpopular  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the books.csv file\n",
    "books = pd.read_csv('books.csv')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15719 entries, 0 to 15718\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   title               15719 non-null  object \n",
      " 1   price               15719 non-null  float64\n",
      " 2   review/helpfulness  15719 non-null  object \n",
      " 3   review/summary      15718 non-null  object \n",
      " 4   review/text         15719 non-null  object \n",
      " 5   description         15719 non-null  object \n",
      " 6   authors             15719 non-null  object \n",
      " 7   categories          15719 non-null  object \n",
      " 8   popularity          15719 non-null  object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check the info of the books dataframe\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                 0\n",
       "price                 0\n",
       "review/helpfulness    0\n",
       "review/summary        1\n",
       "review/text           0\n",
       "description           0\n",
       "authors               0\n",
       "categories            0\n",
       "popularity            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null\n",
    "books.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null\n",
    "books = books.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis\n",
    "# Check the distribution of the price\n",
    "plt.hist(books['price'])\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile report\n",
    "profile = ProfileReport(books, title='Pandas Profiling Report', explorative=True)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicates\n",
    "books.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and reset index\n",
    "books = books.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12424 entries, 0 to 12423\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   title               12424 non-null  object \n",
      " 1   price               12424 non-null  float64\n",
      " 2   review/helpfulness  12424 non-null  object \n",
      " 3   review/summary      12424 non-null  object \n",
      " 4   review/text         12424 non-null  object \n",
      " 5   description         12424 non-null  object \n",
      " 6   authors             12424 non-null  object \n",
      " 7   categories          12424 non-null  object \n",
      " 8   popularity          12424 non-null  object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 873.7+ KB\n"
     ]
    }
   ],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6662 entries, 0 to 6661\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     6662 non-null   object \n",
      " 1   price                     6662 non-null   float64\n",
      " 2   review/helpfulness        6662 non-null   object \n",
      " 3   review/summary            6662 non-null   object \n",
      " 4   review/text               6662 non-null   object \n",
      " 5   description               6662 non-null   object \n",
      " 6   authors                   6662 non-null   object \n",
      " 7   categories                6662 non-null   object \n",
      " 8   popularity                6662 non-null   object \n",
      " 9   review_helpfulness_ratio  6662 non-null   float64\n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 520.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data further by incorporating price and review/helpfulness\n",
    "books['review_helpfulness_ratio'] = books['review/helpfulness'].apply(lambda x: int(x.split('/')[0]) / (int(x.split('/')[1]) + 1))\n",
    "\n",
    "# Keep the records with review/helpfulness ratio greater than 0.5 or total reviews are zero\n",
    "books = books[books['review_helpfulness_ratio'] >= 0.5].reset_index(drop=True)\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need the texts columns and the popularity column\n",
    "df = books[['title', 'review/text', 'description', 'authors', 'categories', 'popularity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\QKmd\\AppData\\Local\\Temp\\ipykernel_13576\\1315723142.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['popularity'] = df['popularity'].map({'Unpopular': 0, 'Popular': 1})\n"
     ]
    }
   ],
   "source": [
    "# Map the popularity to 0 and 1\n",
    "df['popularity'] = df['popularity'].map({'Unpopular': 0, 'Popular': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity\n",
       "Unpopular    3636\n",
       "Popular      3026\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['popularity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the lightning data module\n",
    "class BookDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, max_length=512, batch_size=32, model_name=model_name):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_size = int(0.8 * len(self.df))\n",
    "        val_size = len(self.df) - train_size\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(self.df, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        texts = [item['description'] for item in batch]\n",
    "        labels = [item['popularity'] for item in batch]\n",
    "\n",
    "        encoding = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "        encoding['labels'] = torch.tensor(labels)\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # # We will concatenate the 'title' and 'review/summary' and 'description' columns\n",
    "        # text = row['review/text']\n",
    "        # Use special tokens to separate different parts of the text 'title', 'review/text', 'description', 'authors', 'categories'\n",
    "        text = (f\"{self.tokenizer.sep_token}title{self.tokenizer.sep_token} \" + row['title'] + \n",
    "                f\" {self.tokenizer.sep_token}review/text{self.tokenizer.sep_token} \" + row['review/text'] + \n",
    "                f\" {self.tokenizer.sep_token}description{self.tokenizer.sep_token} \" + row['description'] + \n",
    "                f\" {self.tokenizer.sep_token}authors{self.tokenizer.sep_token} \" + row['authors'] + \n",
    "                f\" {self.tokenizer.sep_token}categories{self.tokenizer.sep_token} \" + row['categories'])\n",
    "        # label = row.get('popularity', 0)  # Default to 0 if 'popularity' is missing\n",
    "        label = row['popularity']\n",
    "        \n",
    "        encoding = self.tokenizer(text, \n",
    "                                  truncation=True, \n",
    "                                  max_length=self.max_length, \n",
    "                                  padding='max_length',\n",
    "                                  return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class BookDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, max_length=512, batch_size=32, model_name=model_name):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BookDataset(self.df, self.tokenizer, self.max_length)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\IT\\qkenv39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data module\n",
    "data_module = BookDataModule(df, max_length=max_length, batch_size=batch_size, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ImbalancedBookDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, max_length=512, batch_size=32, model_name=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.class_weights = None  # Initialize class_weights attribute\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = BookDataset(self.df, self.tokenizer, self.max_length)\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        val_size = len(self.dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(self.dataset, [train_size, val_size])\n",
    "\n",
    "        # Compute class weights\n",
    "        labels = self.df['popularity'].values\n",
    "        unique_labels = np.unique(labels)\n",
    "        class_weights = compute_class_weight('balanced', classes=unique_labels, y=labels)\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Create weighted sampler for training data\n",
    "        train_labels = [self.df['popularity'].iloc[i] for i in self.train_dataset.indices]\n",
    "        weights = [class_weights[unique_labels.tolist().index(label)] for label in train_labels]\n",
    "        self.sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, sampler=self.sampler)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        if self.class_weights is None:\n",
    "            raise ValueError(\"setup() must be called before accessing class_weights\")\n",
    "        return self.class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the data module, call setup() and check the class weights\n",
    "data_module = ImbalancedBookDataModule(df, max_length=max_length, batch_size=batch_size, model_name=model_name)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lightning model\n",
    "class BookModel(pl.LightningModule):\n",
    "    def __init__(self, model_name=model_name, lr=2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, 1)\n",
    "        self.lr = lr\n",
    "        self.accuracy = Accuracy(task='binary')\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # We freeze the model weights and only train the classifier\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the top 3 layer of the model\n",
    "        for param in self.model.encoder.layer[-3:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # # Fully connected layer after CLS token\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(self.model.config.hidden_size, 512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(512, 1)\n",
    "        # )\n",
    "\n",
    "    # # def forward(self, input_ids, attention_mask):\n",
    "    #     outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    #     cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "    #     logits = self.fc(cls_output)\n",
    "\n",
    "    #     return logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask).squeeze()\n",
    "        loss = self.loss(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask).squeeze()\n",
    "        loss = self.loss(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = BookModel(model_name=model_name, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ImbalancedBookClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.lr = lr\n",
    "        self.accuracy = Accuracy(task='binary')\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # We freeze the model weights and only train the classifier\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Fully connected layer after CLS token\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.model.config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(cls_output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask).squeeze()\n",
    "        loss = self.loss(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "\n",
    "        # # Use weighted loss\n",
    "        # loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device))\n",
    "        # loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "        # acc = self.accuracy(logits, labels)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask).squeeze()\n",
    "        loss = self.loss(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the model\n",
    "model = ImbalancedBookClassifier(model_name, data_module.get_class_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min',\n",
    "                                      dirpath='checkpoints',\n",
    "                                      filename='book_model-{epoch:02d}-{val_loss:.2f}',\n",
    "                                      save_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Define the trainer\n",
    "trainer = pl.Trainer(max_epochs=100, devices=-1,\n",
    "                     callbacks=[early_stopping, checkpoint_callback],\n",
    "                     precision='16-mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "f:\\IT\\qkenv39\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory F:\\IT\\DataScience\\Datacamp_Competition\\books\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model      | RobertaModel      | 124 M  | eval \n",
      "1 | classifier | Linear            | 769    | train\n",
      "2 | accuracy   | BinaryAccuracy    | 0      | train\n",
      "3 | loss       | BCEWithLogitsLoss | 0      | train\n",
      "---------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "103 M     Non-trainable params\n",
      "124 M     Total params\n",
      "498.586   Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f752b6a4cefc41baa9d91223e7473c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\IT\\qkenv39\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "f:\\IT\\qkenv39\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcb3e2ee8dc4f6fa8236b18b78b4b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700732d37c874472af694e72d90ff3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae197af0be443dd9ce3e21aecfe0228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.444\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3fab45c5a74a0489205fd11783a11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32e838ba4f44ef4bed56e02b0e13155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21704a713d1e4179b0be56dc08f6cadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b9ee42e322439db594094f752f22fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cb2cf5a8ed4233840151ae707644e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 0.430. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load epoch 15 checkpoint\n",
    "model = BookModel.load_from_checkpoint('checkpoints/book_model-epoch=42-val_loss=0.48.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the model\n",
    "trainer.validate(model, data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = BookModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18547e735edc406da69465c4adbb3fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7891973257064819     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.43004903197288513    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7891973257064819    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.43004903197288513   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': 0.43004903197288513, 'val_acc_epoch': 0.7891973257064819}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the model\n",
    "trainer.validate(best_model, data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'book_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qkenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
